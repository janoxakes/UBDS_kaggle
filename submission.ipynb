{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8226e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jano/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/jano/miniconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98643411 0.98926829 0.98627451 0.98527969 0.98729228]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = None\n",
    "\n",
    "# %pip install nltk sklearn\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "wordnet_lemma  = WordNetLemmatizer()\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "def lemmatize(word, pos='v'):\n",
    "    return wordnet_lemma.lemmatize(word, pos=pos)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def pre_process(text):\n",
    "    text = re.sub('<.*?>', ' ', text) # Remove html tags. Added flag if \"<div\" is found later on process\n",
    "    text = re.sub(r'=\\d\\w',' ',text) # Remove encoded symbols like =2E, =3A....    \n",
    "    return text\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    out = word_tokenize(text)\n",
    "    out = [lemmatize(word,\n",
    "#                      pos=\"v\"\n",
    "                    ) for word in out]\n",
    "    out = [word for word in out if word not in stop]\n",
    "    return out\n",
    "\n",
    "def process_data(df, vectorizer, normalizer, fit=False):\n",
    "    x_text = df['text'].apply(pre_process)\n",
    "    x_len = np.array(df['text'].apply(len)).reshape(-1, 1)\n",
    "    if fit:\n",
    "        vectorizer.fit(x_text)\n",
    "        normalizer.fit(x_len)\n",
    "    x_text = vectorizer.transform(x_text)\n",
    "    x_len = normalizer.transform(x_len)\n",
    "    \n",
    "    x_html = np.array(df['text'].str.contains('<div', case=False)).reshape(-1, 1)\n",
    "    X = hstack((x_text, x_len, x_html))\n",
    "    if 'label' in df:\n",
    "        Y = df['label']\n",
    "    else:\n",
    "        Y = None\n",
    "    return X, Y\n",
    "               \n",
    "    \n",
    "    \n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')#.sample(100)\n",
    "\n",
    "\n",
    "# Transform Data\n",
    "# Change vectorizer for testing if required\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), tokenizer=tokenize_lemmatize)\n",
    "# vectorizer = CountVectorizer(tokenizer=tokenize_lemmatize, ngram_range=(1,4))\n",
    "\n",
    "# Normalizer for the email lenght\n",
    "normalizer = Normalizer()\n",
    "\n",
    "X, Y = process_data(data, vectorizer, normalizer, fit=True)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(cross_validate(clf, X, Y, n_jobs=4, cv=5, scoring='f1')['test_score'])\n",
    "\n",
    "# Train\n",
    "clf.fit(X, Y)\n",
    "\n",
    "test_data = pd.read_csv(\"data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "X_test, Y_test = process_data(test_data, vectorizer, normalizer)\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "\n",
    "df = pd.DataFrame(result, columns=['Category'])\n",
    "df['Id'] = df.index\n",
    "df = df[['Id','Category']]\n",
    "df.to_csv('prediction.csv', index=False)\n",
    "print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adb0d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983232729711603"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = clf.predict(X)\n",
    "# (Y==y_pred).all()\n",
    "# clf.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d64ff-a24c-4ecd-93d9-e0c257d73eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d858b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv(\"data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# X_test, Y_test = process_data(test_data, vectorizer, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fbb44b-5b0c-490e-86a8-b2089a5ca9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ecd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = clf.predict(X_test)\n",
    "\n",
    "# df = pd.DataFrame(result, columns=['Category'])\n",
    "# df['Id'] = df.index\n",
    "# df = df[['Id','Category']]\n",
    "# df.to_csv('prediction.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd02304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jano/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 39.7k/39.7k [00:02<00:00, 14.9kB/s]\n",
      "Successfully submitted to Fraudulent E-mails: Spam or Ham?"
     ]
    }
   ],
   "source": [
    "# Uncomment for submission\n",
    "!kaggle competitions submit -c dsub-fraudulentemails -f prediction.csv -m \"Message\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
